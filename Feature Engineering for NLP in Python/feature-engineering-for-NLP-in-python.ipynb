{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic features and readability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('russian_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127447</td>\n",
       "      <td>LIVE STREAM VIDEO=&gt; Donald Trump Rallies in Co...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123642</td>\n",
       "      <td>Muslim Attacks NYPD Cops with Meat Cleaver. Me...</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226970</td>\n",
       "      <td>.@vfpatlas well that's a swella word there (di...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138339</td>\n",
       "      <td>RT wehking_pamela: Bobby_Axelrod2k MMFlint don...</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161610</td>\n",
       "      <td>Жители обстреливаемых районов Донецка проводят...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content  char_count\n",
       "0      127447  LIVE STREAM VIDEO=> Donald Trump Rallies in Co...         130\n",
       "1      123642  Muslim Attacks NYPD Cops with Meat Cleaver. Me...         138\n",
       "2      226970  .@vfpatlas well that's a swella word there (di...          65\n",
       "3      138339  RT wehking_pamela: Bobby_Axelrod2k MMFlint don...         139\n",
       "4      161610  Жители обстреливаемых районов Донецка проводят...         131"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.462\n"
     ]
    }
   ],
   "source": [
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your information, there is research that indicates that fake news articles tend to have longer titles! Therefore, even extremely basic features such as character counts can prove to be very useful in certain applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted = pd.read_csv('ted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We're going to talk — my — a new lecture, just...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a representation of your brain, and yo...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a great honor today to share with you The...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My passions are music, technology and making t...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It used to be that if you wanted to get a comp...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  We're going to talk — my — a new lecture, just...   \n",
       "1  This is a representation of your brain, and yo...   \n",
       "2  It's a great honor today to share with you The...   \n",
       "3  My passions are music, technology and making t...   \n",
       "4  It used to be that if you wanted to get a comp...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987.1\n"
     ]
    }
   ],
   "source": [
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAU9UlEQVR4nO3dfbRldX3f8fdHRuRhlEFwTWCG5dBKNARqIyOirNqL2BRFHJqllIQoGOy0jQ8YsUJcbbVpVoutSICoXVMwIUvigIQlqCTVBdxm0QQanhbDgy5G5GGGgeFxcEAilG//OHuSw/XeuYd7z50z5+f7tdasu/f+7b1/v+++l8/d53fO3aSqkCS15WWjHoAkafgMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnumpckleR1ox7HzizJZJIPd8snJfnuEM99R5KJbvlzSb42xHN/JskFwzqfdizD/edAknuTvHPKtlOSXLfA/S54Hztakj9O8vtzPb6qLq6qXx1WP1X1y1U1Odfx9PU3kWTDlHP/l6r68HzPrdEw3KUxlGTRqMegnZvhLgCSnJnkh0l+nOTOJP+ir+11Sf53ki1JHk1yyZTD35nk7iRPJvlSen4J+B/AW5NsTfJkd65jk9yS5KkkDyT53JRxfDDJfUkeS/IfpnvV0bfv7knO7vbfkuS6JLt3be/tpiye7KZFfqnvuBdNJfXfJW+7g01yepLNSTYl+VDXtho4Cfh0V9O3ZhjXP0vy/W5Mfwikr+3vXs101+mcrp+nkqxLcshM/XTX4owktwFPJ1k0zfXZLckl3ffx5iRvnK3uJHsCfw7s3/W3Ncn+U6d5Zrmm9yb5VJLburovSbLbdNdHO4bhrm1+CPwTYC/gPwFfS7Jf1/afge8CewPLgfOnHPse4M3APwJOAP55Vd0F/Bvgr6tqcVUt6fZ9GvggsAQ4Fvi3SY4HSHIw8GV6wbZfN5Zl2xnzF4DDgLcBrwY+DbyQ5BeBrwOfAF4DXAV8K8muA16LX+jr+1TgS0n2rqo1wMXAf+tqOm7qgUn2BS4H/j2wL73reuQM/fwq8HbgF7v+TgAem6WfX6d33ZZU1fPTnHMV8I3uevwp8M0kL99esVX1NPAu4MGuv8VV9eCUuga5picAxwAH0vtZOGV7/WphGe4/P77Z3XE92d1Ff7m/saq+UVUPVtULVXUJcDdweNf8HPBaYP+qeraqps6jn1VVT1bV/cC1wD+eaRBVNVlV67p+bqMXGP+0a34f8K2quq6qfgr8R2Dahx8leRnwW8BpVbWxqv5fVf1VVf0t8C+B71TV96rqOXq/BHan90tgEM8Bv1dVz1XVVcBW4PUDHvtu4I6quqzr+w+Ah7bTzyuBNwCpqruqatMs5z+vqh6oqp/M0H5TX99fBHYDjhhw7NszyDU9r/sZehz4Ftv5OdDCM9x/fhxfVUu2/QN+u7+xmw65tS/8D6F35wm9O+IA/7d7Wf5bU87dH17PAItnGkSStyS5NskjSbbQu7vf1s/+wAPb9q2qZ4DHZjjVvvSC64fTtO0P3Nd3nhe6827vVUC/x6bcFW+3pmn67q+h+tf7VdU1wB8CXwI2J1mT5FWznH/ac03X3tW9oRvTfA1yTQf+OdDCM9xFktcC/xP4KLBPF/63080VV9VDVfWvqmp/4F8DX85gH3+c7q77T4ErgQOqai968/Lb5qQ30Zv22Tau3YF9Zjj3o8CzwD+cpu1Beq80tp0nwAHAxm7TM8Aeffv/wmyF9JntMaqbur6m9j39yarOq6rDgIPpTc/8u1n6ma3//r5fRu96bpti2V7ds513tmuqnYzhLoA96f3H/QhA9wbiIdsak7w/ybbQfaLb94UBzvswsHzKvOwrgcer6tkkhwO/0dd2GXBckrd1x3yOvjcj+3V3jl8Fvti9+bdLkrcmeQVwKXBskqO7+ebTgb8F/qo7/FbgN7pjjuHvp4UG8TDwD7bT/h3gl5P8WnqfaPk4M/zySPLm7pXMy+m9F/Esf39dZ+tnJof19f0JenVf37Vtr+6HgX2S7DXDeWe7ptrJGO6iqu4Ezgb+mt5/5IcC/6dvlzcDNyTZSu+u+7SqumeAU18D3AE8lOTRbttvA7+X5Mf05tQv7RvHHcDHgLX07oC3Apvphch0PgWsA/4GeBz4PPCyqvoB8Jv03vh9FDgOOK6bxwc4rdv2JL03b785QC3bXAgc3E1f/cxxVfUo8H7gLHpTSgfx4mvZ71X0XjE9QW/K4zHgvw/Sz3ZcQW9+/AngA8CvdXPksJ26q+r79N7/uKfr80VTOQNcU+1k4v+sQzurJIvpBdFBVfWjUY9HGifeuWunkuS4JHt0n73+Ar0783tHOypp/Bju2tmsovfm3YP0pjROLF9eSi+Z0zKS1CDv3CWpQTvFw4f23XffWrFixZyOffrpp9lzzz2HO6CdSMv1Wdv4arm+cartpptuerSqXjNd204R7itWrODGG2+c07GTk5NMTEwMd0A7kZbrs7bx1XJ941RbkvtmanNaRpIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGrRT/IXqfKzbuIVTzvzOSPq+96xjR9KvJM3GO3dJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDRoo3JP8TpI7ktye5OtJdktyYJIbkqxPckmSXbt9X9Gtr+/aVyxkAZKknzVruCdZBnwcWFlVhwC7ACcCnwfOqarXAU8Ap3aHnAo80W0/p9tPkrQDDTotswjYPckiYA9gE/AO4LKu/SLg+G55VbdO1350kgxnuJKkQcwa7lW1EfgCcD+9UN8C3AQ8WVXPd7ttAJZ1y8uAB7pjn+/232e4w5Ykbc+sj/xNsje9u/EDgSeBbwDHzLfjJKuB1QBLly5lcnJyTudZujucfujzs++4AOY65pdi69atO6SfUbC28dVyfa3UNsjz3N8J/KiqHgFIcjlwJLAkyaLu7nw5sLHbfyNwALChm8bZC3hs6kmrag2wBmDlypU1MTExpwLOv/gKzl43msfS33vSxIL3MTk5yVyvzc7O2sZXy/W1Utsgc+73A0ck2aObOz8auBO4Fnhft8/JwBXd8pXdOl37NVVVwxuyJGk2g8y530DvjdGbgXXdMWuAM4BPJllPb079wu6QC4F9uu2fBM5cgHFLkrZjoPmMqvos8Nkpm+8BDp9m32eB989/aJKkufIvVCWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQQOGeZEmSy5J8P8ldSd6a5NVJvpfk7u7r3t2+SXJekvVJbkvypoUtQZI01aB37ucCf1FVbwDeCNwFnAlcXVUHAVd36wDvAg7q/q0GvjLUEUuSZjVruCfZC3g7cCFAVf20qp4EVgEXdbtdBBzfLa8C/qR6rgeWJNlv6COXJM1okDv3A4FHgD9KckuSC5LsCSytqk3dPg8BS7vlZcADfcdv6LZJknaQVNX2d0hWAtcDR1bVDUnOBZ4CPlZVS/r2e6Kq9k7ybeCsqrqu2341cEZV3TjlvKvpTduwdOnSw9auXTunAjY/voWHfzKnQ+ft0GV7LXgfW7duZfHixQvezyhY2/hqub5xqu2oo466qapWTte2aIDjNwAbquqGbv0yevPrDyfZr6o2ddMum7v2jcABfccv77a9SFWtAdYArFy5siYmJgap5Wecf/EVnL1ukDKG796TJha8j8nJSeZ6bXZ21ja+Wq6vldpmnZapqoeAB5K8vtt0NHAncCVwcrftZOCKbvlK4IPdp2aOALb0Td9IknaAQW95PwZcnGRX4B7gQ/R+MVya5FTgPuCEbt+rgHcD64Fnun0lSTvQQOFeVbcC083rHD3NvgV8ZJ7jkiTNg3+hKkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lq0MDhnmSXJLck+Xa3fmCSG5KsT3JJkl277a/o1td37SsWZuiSpJm8lDv304C7+tY/D5xTVa8DngBO7bafCjzRbT+n20+StAMNFO5JlgPHAhd06wHeAVzW7XIRcHy3vKpbp2s/uttfkrSDpKpm3ym5DPivwCuBTwGnANd3d+ckOQD486o6JMntwDFVtaFr+yHwlqp6dMo5VwOrAZYuXXrY2rVr51TA5se38PBP5nTovB26bK8F72Pr1q0sXrx4wfsZBWsbXy3XN061HXXUUTdV1crp2hbNdnCS9wCbq+qmJBPDGlRVrQHWAKxcubImJuZ26vMvvoKz181axoK496SJBe9jcnKSuV6bnZ21ja+W62ultkFS8UjgvUneDewGvAo4F1iSZFFVPQ8sBzZ2+28EDgA2JFkE7AU8NvSRS5JmNOuce1X9blUtr6oVwInANVV1EnAt8L5ut5OBK7rlK7t1uvZrapC5H0nS0Mznc+5nAJ9Msh7YB7iw234hsE+3/ZPAmfMboiTppXpJk9VVNQlMdsv3AIdPs8+zwPuHMDZJ0hz5F6qS1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1aNZwT3JAkmuT3JnkjiSnddtfneR7Se7uvu7dbU+S85KsT3JbkjctdBGSpBcb5M79eeD0qjoYOAL4SJKDgTOBq6vqIODqbh3gXcBB3b/VwFeGPmpJ0nbNGu5Vtamqbu6WfwzcBSwDVgEXdbtdBBzfLa8C/qR6rgeWJNlv6COXJM0oVTX4zskK4C+BQ4D7q2pJtz3AE1W1JMm3gbOq6rqu7WrgjKq6ccq5VtO7s2fp0qWHrV27dk4FbH58Cw//ZE6Hztuhy/Za8D62bt3K4sWLF7yfUbC28dVyfeNU21FHHXVTVa2crm3RoCdJshj4M+ATVfVUL897qqqSDP5bonfMGmANwMqVK2tiYuKlHP53zr/4Cs5eN3AZQ3XvSRML3sfk5CRzvTY7O2sbXy3X10ptA31aJsnL6QX7xVV1ebf54W3TLd3Xzd32jcABfYcv77ZJknaQQT4tE+BC4K6q+mJf05XAyd3yycAVfds/2H1q5ghgS1VtGuKYJUmzGGQ+40jgA8C6JLd22z4DnAVcmuRU4D7ghK7tKuDdwHrgGeBDQx2xJGlWs4Z798ZoZmg+epr9C/jIPMclSZoH/0JVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYtGvUAxtmKM7+z4H2cfujznDKln3vPOnbB+5U03rxzl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGuSzZcbQjnimzUx8ro00Hrxzl6QGGe6S1CDDXZIatCBz7kmOAc4FdgEuqKqzFqIf7XjDnO+f7ln1OxvfY9C4Gvqde5JdgC8B7wIOBn49ycHD7keSNLOFuHM/HFhfVfcAJFkLrALuXIC+pAU111cq4/CqZCY/j69W+r/PO/p7t1DXO1U13BMm7wOOqaoPd+sfAN5SVR+dst9qYHW3+nrgB3Pscl/g0TkeOw5ars/axlfL9Y1Tba+tqtdM1zCyz7lX1RpgzXzPk+TGqlo5hCHtlFquz9rGV8v1tVLbQnxaZiNwQN/68m6bJGkHWYhw/xvgoCQHJtkVOBG4cgH6kSTNYOjTMlX1fJKPAv+L3kchv1pVdwy7nz7zntrZybVcn7WNr5bra6K2ob+hKkkaPf9CVZIaZLhLUoPGOtyTHJPkB0nWJzlz1OMZliQHJLk2yZ1J7khy2qjHNGxJdklyS5Jvj3osw5ZkSZLLknw/yV1J3jrqMQ1Lkt/pfiZvT/L1JLuNekzzkeSrSTYnub1v26uTfC/J3d3XvUc5xrka23Bv/DEHzwOnV9XBwBHARxqqbZvTgLtGPYgFci7wF1X1BuCNNFJnkmXAx4GVVXUIvQ9MnDjaUc3bHwPHTNl2JnB1VR0EXN2tj52xDXf6HnNQVT8Ftj3mYOxV1aaqurlb/jG9cFg22lENT5LlwLHABaMey7Al2Qt4O3AhQFX9tKqeHO2ohmoRsHuSRcAewIMjHs+8VNVfAo9P2bwKuKhbvgg4focOakjGOdyXAQ/0rW+goQDcJskK4FeAG0Y7kqH6A+DTwAujHsgCOBB4BPijbtrpgiR7jnpQw1BVG4EvAPcDm4AtVfXd0Y5qQSytqk3d8kPA0lEOZq7GOdybl2Qx8GfAJ6rqqVGPZxiSvAfYXFU3jXosC2QR8CbgK1X1K8DTjOnL+qm6uedV9H6B7Q/smeQ3RzuqhVW9z4qP5efFxzncm37MQZKX0wv2i6vq8lGPZ4iOBN6b5F56U2nvSPK10Q5pqDYAG6pq2yuty+iFfQveCfyoqh6pqueAy4G3jXhMC+HhJPsBdF83j3g8czLO4d7sYw6ShN6c7V1V9cVRj2eYqup3q2p5Va2g9z27pqqaufurqoeAB5K8vtt0NO087vp+4Igke3Q/o0fTyJvFU1wJnNwtnwxcMcKxzNnIngo5XyN4zMGOdCTwAWBdklu7bZ+pqqtGOCYN7mPAxd1Nxz3Ah0Y8nqGoqhuSXAbcTO8TXbcw5n+qn+TrwASwb5INwGeBs4BLk5wK3AecMLoRzp2PH5CkBo3ztIwkaQaGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQ/we3WG/fBRfvGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function that returns number of mentions in a string\n",
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flesch reading ease measures how complex a text is. **The lower the score, the more difficult the text is to read**. The Flesch readability score uses the average length of your sentences (measured by the number of words) and the average number of syllables per word in an equation to calculate the reading ease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greater the average sentence length, harder the text is to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greater the average number of syllables in a word, harder the text is to read\n",
    "- I live in my home\n",
    "- I reside in my  domicile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher the score, greater the readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gunning fog index\n",
    "\n",
    "- developed in 1954\n",
    "- also dependent on average sentence length\n",
    "- greater the percentage of complex words, harder the text is to read\n",
    "- higher the index, lesser the readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textatistic\n",
      "  Downloading https://files.pythonhosted.org/packages/73/f8/74dade1df8998ce9a42cba21b3cdf284f3f273e7e22fbcab27955d213e61/textatistic-0.0.1.tar.gz\n",
      "Collecting pyhyphen>=2.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/e7/97ff5a575bd0744c67b28316fccbbcf180806bb89424df94fca9c1f6ea8c/PyHyphen-3.0.1.tar.gz\n",
      "Collecting appdirs\n",
      "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in ./venv/lib/python3.7/site-packages (from pyhyphen>=2.0.5->textatistic) (1.13.0)\n",
      "Building wheels for collected packages: textatistic, pyhyphen\n",
      "  Building wheel for textatistic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for textatistic: filename=textatistic-0.0.1-cp37-none-any.whl size=29056 sha256=df359075b493a383a1e707ab0a07ae0349f04810931e6ae25cdab0d84d70f74c\n",
      "  Stored in directory: /Users/hakan/Library/Caches/pip/wheels/1d/ec/34/69c3cae349149cd91552c4c470efcbd08bbd21ba30b12e08ab\n",
      "  Building wheel for pyhyphen (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyhyphen: filename=PyHyphen-3.0.1-cp37-cp37m-macosx_10_14_x86_64.whl size=30442 sha256=54cab1c52292ab8775b79191c862a6264e769440678769a78c139259579e0bfc\n",
      "  Stored in directory: /Users/hakan/Library/Caches/pip/wheels/85/46/93/46c556b5f054568b7470c86c4f76ea628a9a8bdf5a355b9c63\n",
      "Successfully built textatistic pyhyphen\n",
      "Installing collected packages: appdirs, pyhyphen, textatistic\n",
      "Successfully installed appdirs-1.4.3 pyhyphen-3.0.1 textatistic-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textatistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sisyphus_essay.txt', 'r') as f:\n",
    "    sisyphus_essay = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Flesch Reading Ease is 81.67\n"
     ]
    }
   ],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# Compute the readability scores \n",
    "readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the essay is at the readability level of a 6th grade American student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flesch_score': 81.67466335836913,\n",
       " 'fleschkincaid_score': 5.485083154506439,\n",
       " 'gunningfog_score': 7.913698140200286,\n",
       " 'smog_score': 8.110721755262034,\n",
       " 'dalechall_score': 7.487433762517882}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readability_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Textatistic(sisyphus_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_count': 6403,\n",
       " 'word_count': 1398,\n",
       " 'sent_count': 96,\n",
       " 'sybl_count': 1824,\n",
       " 'notdalechall_count': 277,\n",
       " 'polysyblword_count': 73}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability of various publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('forbes.txt', 'r'); forbes = f.readline(); f.close();\n",
    "f = open('harvard_law.txt', 'r'); harvard_law = f.readline(); f.close();\n",
    "f = open('r_digest.txt', 'r'); r_digest = f.readline(); f.close();\n",
    "f = open('time_kids.txt', 'r'); time_kids = f.readline(); f.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.436002482929858, 20.735401069518716, 11.085587583148559, 5.926785009861934]\n"
     ]
    }
   ],
   "source": [
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "    readability_scores = Textatistic(excerpt).scores\n",
    "    gunning_fog = readability_scores['gunningfog_score']\n",
    "    gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Harvard Law Review excerpt has the highest Gunning fog index; indicating that it can be comprehended only by readers who have graduated college. On the other hand, the Time for Kids article, intended for children, has a much lower fog index and can be comprehended by 5th grade students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing, POS tagging and NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0MB 381kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in ./venv/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (45.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in ./venv/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in ./venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.4.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: more-itertools in ./venv/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (8.1.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.5-cp37-none-any.whl size=12011740 sha256=9c6e1caa79532df88a79f3f8fa89f21182ae72eaaf17d24ccc850bf04f5d4d91\n",
      "  Stored in directory: /private/var/folders/gt/jd9v6_wj1398xf69593kqr700000gn/T/pip-ephem-wheel-cache-773wwrw1/wheels/6a/47/fb/6b5a0b8906d8e8779246c67d4658fd8a544d4a03a75520197a\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gettysburg.txt', 'r') as f:\n",
    "    gettysburg = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Four',\n",
       " 'score',\n",
       " 'and',\n",
       " 'seven',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'our',\n",
       " 'fathers',\n",
       " 'brought',\n",
       " 'forth',\n",
       " 'on',\n",
       " 'this',\n",
       " 'continent',\n",
       " ',',\n",
       " 'a',\n",
       " 'new',\n",
       " 'nation',\n",
       " ',',\n",
       " 'conceived',\n",
       " 'in',\n",
       " 'Liberty',\n",
       " ',',\n",
       " 'and',\n",
       " 'dedicated',\n",
       " 'to',\n",
       " 'the',\n",
       " 'proposition',\n",
       " 'that',\n",
       " 'all',\n",
       " 'men',\n",
       " 'are',\n",
       " 'created',\n",
       " 'equal',\n",
       " '.',\n",
       " 'Now',\n",
       " 'we',\n",
       " \"'re\",\n",
       " 'engaged',\n",
       " 'in',\n",
       " 'a',\n",
       " 'great',\n",
       " 'civil',\n",
       " 'war',\n",
       " ',',\n",
       " 'testing',\n",
       " 'whether',\n",
       " 'that',\n",
       " 'nation',\n",
       " ',',\n",
       " 'or',\n",
       " 'any',\n",
       " 'nation',\n",
       " 'so',\n",
       " 'conceived',\n",
       " 'and',\n",
       " 'so',\n",
       " 'dedicated',\n",
       " ',',\n",
       " 'can',\n",
       " 'long',\n",
       " 'endure',\n",
       " '.',\n",
       " 'We',\n",
       " \"'re\",\n",
       " 'met',\n",
       " 'on',\n",
       " 'a',\n",
       " 'great',\n",
       " 'battlefield',\n",
       " 'of',\n",
       " 'that',\n",
       " 'war',\n",
       " '.',\n",
       " 'We',\n",
       " \"'ve\",\n",
       " 'come',\n",
       " 'to',\n",
       " 'dedicate',\n",
       " 'a',\n",
       " 'portion',\n",
       " 'of',\n",
       " 'that',\n",
       " 'field',\n",
       " ',',\n",
       " 'as',\n",
       " 'a',\n",
       " 'final',\n",
       " 'resting',\n",
       " 'place',\n",
       " 'for',\n",
       " 'those',\n",
       " 'who',\n",
       " 'here',\n",
       " 'gave',\n",
       " 'their',\n",
       " 'lives',\n",
       " 'that',\n",
       " 'that',\n",
       " 'nation',\n",
       " 'might',\n",
       " 'live',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'altogether',\n",
       " 'fitting',\n",
       " 'and',\n",
       " 'proper',\n",
       " 'that',\n",
       " 'we',\n",
       " 'should',\n",
       " 'do',\n",
       " 'this',\n",
       " '.',\n",
       " 'But',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'larger',\n",
       " 'sense',\n",
       " ',',\n",
       " 'we',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'dedicate',\n",
       " '-',\n",
       " 'we',\n",
       " 'can',\n",
       " 'not',\n",
       " 'consecrate',\n",
       " '-',\n",
       " 'we',\n",
       " 'can',\n",
       " 'not',\n",
       " 'hallow',\n",
       " '-',\n",
       " 'this',\n",
       " 'ground',\n",
       " '.',\n",
       " 'The',\n",
       " 'brave',\n",
       " 'men',\n",
       " ',',\n",
       " 'living',\n",
       " 'and',\n",
       " 'dead',\n",
       " ',',\n",
       " 'who',\n",
       " 'struggled',\n",
       " 'here',\n",
       " ',',\n",
       " 'have',\n",
       " 'consecrated',\n",
       " 'it',\n",
       " ',',\n",
       " 'far',\n",
       " 'above',\n",
       " 'our',\n",
       " 'poor',\n",
       " 'power',\n",
       " 'to',\n",
       " 'add',\n",
       " 'or',\n",
       " 'detract',\n",
       " '.',\n",
       " 'The',\n",
       " 'world',\n",
       " 'will',\n",
       " 'little',\n",
       " 'note',\n",
       " ',',\n",
       " 'nor',\n",
       " 'long',\n",
       " 'remember',\n",
       " 'what',\n",
       " 'we',\n",
       " 'say',\n",
       " 'here',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'can',\n",
       " 'never',\n",
       " 'forget',\n",
       " 'what',\n",
       " 'they',\n",
       " 'did',\n",
       " 'here',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'for',\n",
       " 'us',\n",
       " 'the',\n",
       " 'living',\n",
       " ',',\n",
       " 'rather',\n",
       " ',',\n",
       " 'to',\n",
       " 'be',\n",
       " 'dedicated',\n",
       " 'here',\n",
       " 'to',\n",
       " 'the',\n",
       " 'unfinished',\n",
       " 'work',\n",
       " 'which',\n",
       " 'they',\n",
       " 'who',\n",
       " 'fought',\n",
       " 'here',\n",
       " 'have',\n",
       " 'thus',\n",
       " 'far',\n",
       " 'so',\n",
       " 'nobly',\n",
       " 'advanced',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'rather',\n",
       " 'for',\n",
       " 'us',\n",
       " 'to',\n",
       " 'be',\n",
       " 'here',\n",
       " 'dedicated',\n",
       " 'to',\n",
       " 'the',\n",
       " 'great',\n",
       " 'task',\n",
       " 'remaining',\n",
       " 'before',\n",
       " 'us',\n",
       " '-',\n",
       " 'that',\n",
       " 'from',\n",
       " 'these',\n",
       " 'honored',\n",
       " 'dead',\n",
       " 'we',\n",
       " 'take',\n",
       " 'increased',\n",
       " 'devotion',\n",
       " 'to',\n",
       " 'that',\n",
       " 'cause',\n",
       " 'for',\n",
       " 'which',\n",
       " 'they',\n",
       " 'gave',\n",
       " 'the',\n",
       " 'last',\n",
       " 'full',\n",
       " 'measure',\n",
       " 'of',\n",
       " 'devotion',\n",
       " '-',\n",
       " 'that',\n",
       " 'we',\n",
       " 'here',\n",
       " 'highly',\n",
       " 'resolve',\n",
       " 'that',\n",
       " 'these',\n",
       " 'dead',\n",
       " 'shall',\n",
       " 'not',\n",
       " 'have',\n",
       " 'died',\n",
       " 'in',\n",
       " 'vain',\n",
       " '-',\n",
       " 'that',\n",
       " 'this',\n",
       " 'nation',\n",
       " ',',\n",
       " 'under',\n",
       " 'God',\n",
       " ',',\n",
       " 'shall',\n",
       " 'have',\n",
       " 'a',\n",
       " 'new',\n",
       " 'birth',\n",
       " 'of',\n",
       " 'freedom',\n",
       " '-',\n",
       " 'and',\n",
       " 'that',\n",
       " 'government',\n",
       " 'of',\n",
       " 'the',\n",
       " 'people',\n",
       " ',',\n",
       " 'by',\n",
       " 'the',\n",
       " 'people',\n",
       " ',',\n",
       " 'for',\n",
       " 'the',\n",
       " 'people',\n",
       " ',',\n",
       " 'shall',\n",
       " 'not',\n",
       " 'perish',\n",
       " 'from',\n",
       " 'the',\n",
       " 'earth',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing the Gettysburg address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now -PRON- be engage in a great civil war , test whether that nation , or any nation so conceive and so dedicated , can long endure . -PRON- be meet on a great battlefield of that war . -PRON- have come to dedicate a portion of that field , as a final resting place for those who here give -PRON- life that that nation may live . -PRON- be altogether fitting and proper that -PRON- should do this . but , in a large sense , -PRON- can not dedicate - -PRON- can not consecrate - -PRON- can not hallow - this ground . the brave man , living and dead , who struggle here , have consecrate -PRON- , far above -PRON- poor power to add or detract . the world will little note , nor long remember what -PRON- say here , but -PRON- can never forget what -PRON- do here . -PRON- be for -PRON- the living , rather , to be dedicate here to the unfinished work which -PRON- who fight here have thus far so nobly advanced . -PRON- be rather for -PRON- to be here dedicate to the great task remain before -PRON- - that from these honor dead -PRON- take increase devotion to that cause for which -PRON- give the last full measure of devotion - that -PRON- here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth .\n"
     ]
    }
   ],
   "source": [
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('blog.txt', 'r') as f:\n",
    "    blog = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "century politic witness alarming rise populism Europe warning sign come UK Brexit Referendum vote swinging way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include alternative Germany AfD win seat enter Bundestag upset Germany political order time Second World War success Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning TED talks in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted = pd.read_csv('ted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We're going to talk — my — a new lecture, just...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a representation of your brain, and yo...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a great honor today to share with you The...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My passions are music, technology and making t...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It used to be that if you wanted to get a comp...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  We're going to talk — my — a new lecture, just...   \n",
       "1  This is a representation of your brain, and yo...   \n",
       "2  It's a great honor today to share with you The...   \n",
       "3  My passions are music, technology and making t...   \n",
       "4  It used to be that if you wanted to get a comp...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      talk new lecture TED illusion create TED try r...\n",
      "1      representation brain brain break left half log...\n",
      "2      great honor today share Digital Universe creat...\n",
      "3      passion music technology thing combination thi...\n",
      "4      use want computer new program programming requ...\n",
      "                             ...                        \n",
      "495    today unpack example iconic design perfect sen...\n",
      "496    brother belong demographic Pat percent accord ...\n",
      "497    John Hockenberry great Tom want start question...\n",
      "498    right moment kill More car internet little mob...\n",
      "499    real problem math education right basically ha...\n",
      "Name: transcript, Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "ted['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotf =  \"He found himself understanding the wearisomeness of this life,\"+\\\n",
    "        \"where every path was an improvisation and a considerable part\" +\\\n",
    "        \"of one’s waking life was spent watching one’s feet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('every', 'DET'), ('path', 'NOUN'), ('was', 'AUX'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), ('partof', 'NOUN'), ('one', 'NOUN'), ('’s', 'PART'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'AUX'), ('spent', 'VERB'), ('watching', 'VERB'), ('one', 'PRON'), ('’s', 'PART'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting nouns in a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun usage in fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.read_csv('fakenews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns(text, model=nlp):    \n",
    "    doc = model(text)    \n",
    "    pos = [token.pos_ for token in doc]\n",
    "    return pos.count('NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_nouns(text, model=nlp):\n",
    "    doc = model(text)\n",
    "    pos = [token.pos_ for token in doc]    \n",
    "    return pos.count('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of proper nouns in real and fake headlines are 2.37 and 4.35 respectively\n",
      "Mean no. of other nouns in real and fake headlines are 2.32 and 1.84 respectively\n"
     ]
    }
   ],
   "source": [
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entities in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sundar Pichai PERSON\n",
      "Google ORG\n",
      "Mountain View GPE\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc instance \n",
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying people mentioned in a news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = \"It’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sheryl Sandberg', 'Mark Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "def find_persons(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Identify the persons\n",
    "    persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "\n",
    "    # Return persons\n",
    "    return persons\n",
    "\n",
    "print(find_persons(tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('movie_overviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9094</th>\n",
       "      <td>159550</td>\n",
       "      <td>The Last Brickmaker in America</td>\n",
       "      <td>A man must cope with the loss of his wife and ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9095</th>\n",
       "      <td>392572</td>\n",
       "      <td>Rustom</td>\n",
       "      <td>Rustom Pavri, an honourable officer of the Ind...</td>\n",
       "      <td>Decorated Officer. Devoted Family Man. Defendi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9096</th>\n",
       "      <td>402672</td>\n",
       "      <td>Mohenjo Daro</td>\n",
       "      <td>Village lad Sarman is drawn to big, bad Mohenj...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9097</th>\n",
       "      <td>315011</td>\n",
       "      <td>Shin Godzilla</td>\n",
       "      <td>From the mind behind Evangelion comes a hit la...</td>\n",
       "      <td>A god incarnate. A city doomed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9098</th>\n",
       "      <td>391698</td>\n",
       "      <td>The Beatles: Eight Days a Week - The Touring Y...</td>\n",
       "      <td>The band stormed Europe in 1963, and, in 1964,...</td>\n",
       "      <td>The band you know. The story you don't.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9099 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0        862                                          Toy Story   \n",
       "1       8844                                            Jumanji   \n",
       "2      15602                                   Grumpier Old Men   \n",
       "3      31357                                  Waiting to Exhale   \n",
       "4      11862                        Father of the Bride Part II   \n",
       "...      ...                                                ...   \n",
       "9094  159550                     The Last Brickmaker in America   \n",
       "9095  392572                                             Rustom   \n",
       "9096  402672                                       Mohenjo Daro   \n",
       "9097  315011                                      Shin Godzilla   \n",
       "9098  391698  The Beatles: Eight Days a Week - The Touring Y...   \n",
       "\n",
       "                                               overview  \\\n",
       "0     Led by Woody, Andy's toys live happily in his ...   \n",
       "1     When siblings Judy and Peter discover an encha...   \n",
       "2     A family wedding reignites the ancient feud be...   \n",
       "3     Cheated on, mistreated and stepped on, the wom...   \n",
       "4     Just when George Banks has recovered from his ...   \n",
       "...                                                 ...   \n",
       "9094  A man must cope with the loss of his wife and ...   \n",
       "9095  Rustom Pavri, an honourable officer of the Ind...   \n",
       "9096  Village lad Sarman is drawn to big, bad Mohenj...   \n",
       "9097  From the mind behind Evangelion comes a hit la...   \n",
       "9098  The band stormed Europe in 1963, and, in 1964,...   \n",
       "\n",
       "                                                tagline  \n",
       "0                                                   NaN  \n",
       "1             Roll the dice and unleash the excitement!  \n",
       "2     Still Yelling. Still Fighting. Still Ready for...  \n",
       "3     Friends are the people who let you be yourself...  \n",
       "4     Just When His World Is Back To Normal... He's ...  \n",
       "...                                                 ...  \n",
       "9094                                                NaN  \n",
       "9095  Decorated Officer. Devoted Family Man. Defendi...  \n",
       "9096                                                NaN  \n",
       "9097                    A god incarnate. A city doomed.  \n",
       "9098            The band you know. The story you don't.  \n",
       "\n",
       "[9099 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/hakan/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074/sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached https://files.pythonhosted.org/packages/82/d9/69769d4f79f3b719cc1255f9bd2b6928c72f43e6f74084e3c67db86c4d2b/scikit_learn-0.22.1-cp37-cp37m-macosx_10_6_intel.whl\n",
      "Collecting scipy>=0.17.0\n",
      "  Using cached https://files.pythonhosted.org/packages/85/7a/ae480be23b768910a9327c33517ced4623ba88dc035f9ce0206657c353a9/scipy-1.4.1-cp37-cp37m-macosx_10_6_intel.whl\n",
      "Requirement already satisfied: numpy>=1.11.0 in ./venv/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl\n",
      "Installing collected packages: scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.14.1 scikit-learn-0.22.1 scipy-1.4.1 sklearn-0.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9099, 7270)\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus['title'])\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing dimensionality and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                        title  \\\n",
       "0    862                    Toy Story   \n",
       "1   8844                      Jumanji   \n",
       "2  15602             Grumpier Old Men   \n",
       "3  31357            Waiting to Exhale   \n",
       "4  11862  Father of the Bride Part II   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...   \n",
       "1  When siblings Judy and Peter discover an encha...   \n",
       "2  A family wedding reignites the ancient feud be...   \n",
       "3  Cheated on, mistreated and stepped on, the wom...   \n",
       "4  Just when George Banks has recovered from his ...   \n",
       "\n",
       "                                             tagline  \n",
       "0                                                NaN  \n",
       "1          Roll the dice and unleash the excitement!  \n",
       "2  Still Yelling. Still Fighting. Still Ready for...  \n",
       "3  Friends are the people who let you be yourself...  \n",
       "4  Just When His World Is Back To Normal... He's ...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(str(text))\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['tagline_lem'] = corpus['tagline'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             nan\n",
       "1    roll dice unleash excitement\n",
       "2           yell fight ready love\n",
       "3    friend people let let forget\n",
       "4      world Normal Surprise life\n",
       "Name: tagline_lem, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['tagline_lem'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_corpus = corpus['tagline_lem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9099, 5213)\n"
     ]
    }
   ],
   "source": [
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n",
    "\n",
    "# Print the shape of bow_lem_matrix\n",
    "print(bow_lem_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping feature indices with feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaargh</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abby</th>\n",
       "      <th>abduction</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>above</th>\n",
       "      <th>abracatastic</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeta</th>\n",
       "      <th>zexy</th>\n",
       "      <th>zhivago</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zorba</th>\n",
       "      <th>zwei</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaargh  aaron  abandon  abby  abduction  ability  able  aboard  above  \\\n",
       "0       0      0        0     0          0        0     0       0      0   \n",
       "1       0      0        0     0          0        0     0       0      0   \n",
       "2       0      0        0     0          0        0     0       0      0   \n",
       "3       0      0        0     0          0        0     0       0      0   \n",
       "4       0      0        0     0          0        0     0       0      0   \n",
       "\n",
       "   abracatastic  ...  zero  zeta  zexy  zhivago  zip  zombie  zone  zoo  \\\n",
       "0             0  ...     0     0     0        0    0       0     0    0   \n",
       "1             0  ...     0     0     0        0    0       0     0    0   \n",
       "2             0  ...     0     0     0        0    0       0     0    0   \n",
       "3             0  ...     0     0     0        0    0       0     0    0   \n",
       "4             0  ...     0     0     0        0    0       0     0    0   \n",
       "\n",
       "   zorba  zwei  \n",
       "0      0     0  \n",
       "1      0     0  \n",
       "2      0     0  \n",
       "3      0     0  \n",
       "4      0     0  \n",
       "\n",
       "[5 rows x 5213 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df = pd.DataFrame(bow_lem_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "bow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW vectors for movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = pd.read_csv('movie_reviews_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this anime series starts out great interesting...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>some may go for a film like this but i most as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i ve seen this piece of perfection during the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this movie is likely the worst movie i ve ever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it ll soon be 10 yrs since this movie was rele...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  this anime series starts out great interesting...          0\n",
       "1  some may go for a film like this but i most as...          0\n",
       "2  i ve seen this piece of perfection during the ...          1\n",
       "3  this movie is likely the worst movie i ve ever...          0\n",
       "4  it ll soon be 10 yrs since this movie was rele...          1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test ,y_train, y_test = train_test_split(movie['review'], movie['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 15038)\n",
      "(250, 15038)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.800\n",
      "\n",
      "Review: The movie was terrible. The music was underwhelming and the acting mediocre.\n",
      "\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "print()\n",
    "print(\"Review: \" + review)\n",
    "print()\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram models for movie tag lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng1, ng2 and ng3 have 17782, 134432 and 322694 features respectively\n"
     ]
    }
   ],
   "source": [
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(movie['review'])\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(movie['review'])\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
    "ng3 = vectorizer_ng3.fit_transform(movie['review'])\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the n-gram range increases, so does the number of features, leading to increased computational costs and a problem known as the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher order n-grams for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ng, X_test_ng, y_train, y_test = train_test_split(ng2,  movie['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.804\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "clf_ng = MultinomialNB()\n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy \n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(vectorizer_ng2.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing performance of n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 0.384 seconds to complete. The accuracy on the test set is 0.75. The ngram representation had 12347 features.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(movie['review'], movie['sentiment'], test_size=0.5, random_state=42, stratify=movie['sentiment'])\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % \\\n",
    "      (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 2.046 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 181165 features.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(movie['review'], movie['sentiment'], test_size=0.5, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % \\\n",
    "      (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted = pd.read_csv('ted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We're going to talk — my — a new lecture, just...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a representation of your brain, and yo...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a great honor today to share with you The...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My passions are music, technology and making t...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It used to be that if you wanted to get a comp...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  We're going to talk — my — a new lecture, just...   \n",
       "1  This is a representation of your brain, and yo...   \n",
       "2  It's a great honor today to share with you The...   \n",
       "3  My passions are music, technology and making t...   \n",
       "4  It used to be that if you wanted to get a comp...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 29158)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted.transcript)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consine score should be between 0 and 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "A = np.array([1,3])\n",
    "B = np.array([-2,2])\n",
    "\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity matrix of a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['The sun is the largest celestial body in the solar system', \n",
    "          'The solar system consists of the sun and eight revolving planets', \n",
    "          'Ra was the Egyptian Sun God', \n",
    "          'The Pyramids were the pinnacle of Egyptian architecture', \n",
    "          'The quick brown fox jumps over the lazy dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.36413198, 0.18314713, 0.18435251, 0.16336438],\n",
       "       [0.36413198, 1.        , 0.15054075, 0.21704584, 0.11203887],\n",
       "       [0.18314713, 0.15054075, 1.        , 0.21318602, 0.07763512],\n",
       "       [0.18435251, 0.21704584, 0.21318602, 1.        , 0.12960089],\n",
       "       [0.16336438, 0.11203887, 0.07763512, 0.12960089, 1.        ]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'architecture',\n",
       " 'body',\n",
       " 'brown',\n",
       " 'celestial',\n",
       " 'consists',\n",
       " 'dog',\n",
       " 'egyptian',\n",
       " 'eight',\n",
       " 'fox',\n",
       " 'god',\n",
       " 'in',\n",
       " 'is',\n",
       " 'jumps',\n",
       " 'largest',\n",
       " 'lazy',\n",
       " 'of',\n",
       " 'over',\n",
       " 'pinnacle',\n",
       " 'planets',\n",
       " 'pyramids',\n",
       " 'quick',\n",
       " 'ra',\n",
       " 'revolving',\n",
       " 'solar',\n",
       " 'sun',\n",
       " 'system',\n",
       " 'the',\n",
       " 'was',\n",
       " 'were']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 27,\n",
       " 'sun': 25,\n",
       " 'is': 12,\n",
       " 'largest': 14,\n",
       " 'celestial': 4,\n",
       " 'body': 2,\n",
       " 'in': 11,\n",
       " 'solar': 24,\n",
       " 'system': 26,\n",
       " 'consists': 5,\n",
       " 'of': 16,\n",
       " 'and': 0,\n",
       " 'eight': 8,\n",
       " 'revolving': 23,\n",
       " 'planets': 19,\n",
       " 'ra': 22,\n",
       " 'was': 28,\n",
       " 'egyptian': 7,\n",
       " 'god': 10,\n",
       " 'pyramids': 20,\n",
       " 'were': 29,\n",
       " 'pinnacle': 18,\n",
       " 'architecture': 1,\n",
       " 'quick': 21,\n",
       " 'brown': 3,\n",
       " 'fox': 9,\n",
       " 'jumps': 13,\n",
       " 'over': 17,\n",
       " 'lazy': 15,\n",
       " 'dog': 6}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['and'], dtype='<U12')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.inverse_transform(['Ra was the Egyptian Sun God'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x30 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 40 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing linear_kernel and cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mlinear_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Compute the linear kernel between X and Y.\n",
       "\n",
       "Read more in the :ref:`User Guide <linear_kernel>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "X : array of shape (n_samples_1, n_features)\n",
       "\n",
       "Y : array of shape (n_samples_2, n_features)\n",
       "\n",
       "dense_output : boolean (optional), default True\n",
       "    Whether to return dense output even when the input is sparse. If\n",
       "    ``False``, the output is sparse if both input arrays are sparse.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "Returns\n",
       "-------\n",
       "Gram matrix : array of shape (n_samples_1, n_samples_2)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Work/Deep-Learning/data-camp/Feature Engineering for NLP in Python/venv/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "linear_kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to linear_kernel to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot recommendation engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie = pd.read_csv('movie_overviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                        title  \\\n",
       "0    862                    Toy Story   \n",
       "1   8844                      Jumanji   \n",
       "2  15602             Grumpier Old Men   \n",
       "3  31357            Waiting to Exhale   \n",
       "4  11862  Father of the Bride Part II   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...   \n",
       "1  When siblings Judy and Peter discover an encha...   \n",
       "2  A family wedding reignites the ancient feud be...   \n",
       "3  Cheated on, mistreated and stepped on, the wom...   \n",
       "4  Just when George Banks has recovered from his ...   \n",
       "\n",
       "                                             tagline  \n",
       "0                                                NaN  \n",
       "1          Roll the dice and unleash the excitement!  \n",
       "2  Still Yelling. Still Fighting. Still Ready for...  \n",
       "3  Friends are the people who let you be yourself...  \n",
       "4  Just When His World Is Back To Normal... He's ...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = df_movie[df_movie.title.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 4)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9099, 4)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hamlet                      5\n",
       "Jane Eyre                   4\n",
       "Carrie                      4\n",
       "The Three Musketeers        4\n",
       "The Phantom of the Opera    4\n",
       "                           ..\n",
       "The Run of the Country      1\n",
       "2 Days in the Valley        1\n",
       "Starred Up                  1\n",
       "BASEketball                 1\n",
       "Tequila Sunrise             1\n",
       "Name: title, Length: 8809, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie.title.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method would be to look at the cast, crew and genre in addition to the plot to generate recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id    title                                           overview  \\\n",
       "1  8844  Jumanji  When siblings Judy and Peter discover an encha...   \n",
       "\n",
       "                                     tagline  \n",
       "1  Roll the dice and unleash the excitement!  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie[df_movie.title.str.contains('Jumanji')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.7/site-packages (0.25.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venv/lib/python3.7/site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in ./venv/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./venv/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.13.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.3'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/ba/f97030b7e8ec0a981abdca173de4e727b3a7b4ed5dba492f362ba87d59a2/pandas-1.0.1-cp37-cp37m-macosx_10_9_x86_64.whl (9.8MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8MB 3.3MB/s eta 0:00:01     |█████████████████████           | 6.4MB 1.6MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in ./venv/lib/python3.7/site-packages (from pandas==1.0.1) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venv/lib/python3.7/site-packages (from pandas==1.0.1) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./venv/lib/python3.7/site-packages (from pandas==1.0.1) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.1) (1.13.0)\n",
      "Installing collected packages: pandas\n",
      "  Found existing installation: pandas 0.25.3\n",
      "    Uninstalling pandas-0.25.3:\n",
      "      Successfully uninstalled pandas-0.25.3\n",
      "Successfully installed pandas-1.0.1\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim, indices, df):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]   \n",
    "    # Get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return df.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = df_movie[df_movie.title.duplicated()].title.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(indices, index=df_movie[df_movie.title.duplicated()].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5741"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices['Batman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.6847505168570327,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim[5741].tolist()[130:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5741                                     Batman\n",
      "7907                           Batman: Year One\n",
      "1242                             Batman & Robin\n",
      "132                              Batman Forever\n",
      "1116                             Batman Returns\n",
      "6150                              Batman Begins\n",
      "7573                 Batman: Under the Red Hood\n",
      "2581               Batman: Mask of the Phantasm\n",
      "8171    Batman: The Dark Knight Returns, Part 1\n",
      "8232    Batman: The Dark Knight Returns, Part 2\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(df_movie.title)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('Batman', cosine_sim,  indices, df_movie['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9099x7056 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17602 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TED talk recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted = pd.read_csv('ted-new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've noticed something interesting about socie...</td>\n",
       "      <td>10 top time-saving tech tips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>I want you to imagine that you're a student in...</td>\n",
       "      <td>The sticky wonder of gecko feet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            transcript  \\\n",
       "0    I've noticed something interesting about socie...   \n",
       "344  I want you to imagine that you're a student in...   \n",
       "\n",
       "                               title  \n",
       "0       10 top time-saving tech tips  \n",
       "344  The sticky wonder of gecko feet  "
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted[ted.transcript.str.contains('noticed something')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = ted.transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim, indices, df):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "    # Get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    talk_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return ted['title'].iloc[talk_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<499x28940 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 234793 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499,)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 499)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>I dedicated the past two years to understandin...</td>\n",
       "      <td>5 ways to kill your dreams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            transcript  \\\n",
       "271  I dedicated the past two years to understandin...   \n",
       "\n",
       "                          title  \n",
       "271  5 ways to kill your dreams  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted[ted.title.str.contains('5 ways to kill your dreams')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ted[~ted.title.duplicated()].title.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(indices, index=ted[~ted.title.duplicated()].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ted[~ted.title.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453             Success is a continuous journey\n",
      "157                        Why we do what we do\n",
      "494                   How to find work you love\n",
      "149          My journey into movies that matter\n",
      "447                        One Laptop per Child\n",
      "230             How to get your ideas to spread\n",
      "497         Plug into your hard-wired happiness\n",
      "495    Why you will fail to have a great career\n",
      "179             Be suspicious of simple stories\n",
      "53                          To upgrade is human\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Generate recommendations \n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices, ted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I 1.0\n",
      "I like 0.111476846\n",
      "I apples 0.0067715086\n",
      "I and -0.117666654\n",
      "I oranges -0.0021670829\n",
      "like I 0.111476846\n",
      "like like 1.0\n",
      "like apples 0.052829094\n",
      "like and -0.15288135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like oranges -0.043253582\n",
      "apples I 0.0067715086\n",
      "apples like 0.052829094\n",
      "apples apples 1.0\n",
      "apples and -0.0011159935\n",
      "apples oranges 0.635426\n",
      "and I -0.117666654\n",
      "and like -0.15288135\n",
      "and apples -0.0011159935\n",
      "and and 1.0\n",
      "and oranges 0.089370795\n",
      "oranges I -0.0021670829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oranges like -0.043253582\n",
      "oranges apples 0.635426\n",
      "oranges and 0.089370795\n",
      "oranges oranges 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "# Create the doc object\n",
    "doc = nlp('I like apples and oranges')\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing similarity of Pink Floyd songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "hopes = \"Beyond the horizon of the place we lived when we were young\\nIn a world of magnets and miracles\\nOur thoughts strayed constantly and without boundary\\nThe ringing of the division bell had begun\\nAlong the Long Road and on down the Causeway\\nDo they still meet there by the Cut\\nThere was a ragged band that followed in our footsteps\\nRunning before times took our dreams away\\nLeaving the myriad small creatures trying to tie us to the ground\\nTo a life consumed by slow decay\\nThe grass was greener\\nThe light was brighter\\nWhen friends surrounded\\nThe nights of wonder\\nLooking beyond the embers of bridges glowing behind us\\nTo a glimpse of how green it was on the other side\\nSteps taken forwards but sleepwalking back again\\nDragged by the force of some in a tide\\nAt a higher altitude with flag unfurled\\nWe reached the dizzy heights of that dreamed of world\\nEncumbered forever by desire and ambition\\nThere's a hunger still unsatisfied\\nOur weary eyes still stray to the horizon\\nThough down this road we've been so many times\\nThe grass was greener\\nThe light was brighter\\nThe taste was sweeter\\nThe nights of wonder\\nWith friends surrounded\\nThe dawn mist glowing\\nThe water flowing\\nThe endless river\\nForever and ever\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "hey = \"Hey you, out there in the cold\\nGetting lonely, getting old\\nCan you feel me?\\nHey you, standing in the aisles\\nWith itchy feet and fading smiles\\nCan you feel me?\\nHey you, don't help them to bury the light\\nDon't give in without a fight\\nHey you out there on your own\\nSitting naked by the phone\\nWould you touch me?\\nHey you with you ear against the wall\\nWaiting for someone to call out\\nWould you touch me?\\nHey you, would you help me to carry the stone?\\nOpen your heart, I'm coming home\\nBut it was only fantasy\\nThe wall was too high\\nAs you can see\\nNo matter how he tried\\nHe could not break free\\nAnd the worms ate into his brain\\nHey you, out there on the road\\nAlways doing what you're told\\nCan you help me?\\nHey you, out there beyond the wall\\nBreaking bottles in the hall\\nCan you help me?\\nHey you, don't tell me there's no hope at all\\nTogether we stand, divided we fall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother = \"\\nMother do you think they'll drop the bomb?\\nMother do you think they'll like this song?\\nMother do you think they'll try to break my balls?\\nOoh, ah\\nMother should I build the wall?\\nMother should I run for President?\\nMother should I trust the government?\\nMother will they put me in the firing mine?\\nOoh ah,\\nIs it just a waste of time?\\nHush now baby, baby, don't you cry.\\nMama's gonna make all your nightmares come true.\\nMama's gonna put all her fears into you.\\nMama's gonna keep you right here under her wing.\\nShe won't let you fly, but she might let you sing.\\nMama's gonna keep baby cozy and warm.\\nOoh baby, ooh baby, ooh baby,\\nOf course mama's gonna help build the wall.\\nMother do you think she's good enough, for me?\\nMother do you think she's dangerous, to me?\\nMother will she tear your little boy apart?\\nOoh ah,\\nMother will she break my heart?\\nHush now baby, baby don't you cry.\\nMama's gonna check out all your girlfriends for you.\\nMama won't let anyone dirty get through.\\nMama's gonna wait up until you get in.\\nMama will always find out where you've been.\\nMama's gonna keep baby healthy and clean.\\nOoh baby, ooh baby, ooh baby,\\nYou'll always be baby to me.\\nMother, did it need to be so high?\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6127587231684104\n",
      "0.9085433676572958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
